# Default OpenNMT parameters from the Lua version.

model_dir: /fs/zisa0/mbehnke/mlp_project/models/tensorflow/baseline+birnn

data:
  train_features_file: /fs/zisa0/mbehnke/mlp_project/data/corpus.bpe.tr
  train_labels_file: /fs/zisa0/mbehnke/mlp_project/data/corpus.bpe.en
  eval_features_file: /fs/zisa0/mbehnke/mlp_project/data/newsdev2016.bpe.tr
  eval_labels_file: /fs/zisa0/mbehnke/mlp_project/data/newsdev2016.bpe.en
  source_words_vocabulary: /fs/zisa0/mbehnke/mlp_project/data/corpus.bpe.tr.vocab
  target_words_vocabulary: /fs/zisa0/mbehnke/mlp_project/data/corpus.bpe.en.vocab

params:
  optimizer: AdamOptimizer
  learning_rate: 0.001
  param_init: 0.1
  clip_gradients: 5.0
  beam_width: 5
  maximum_iterations: 200

train:
  batch_size: 64
  bucket_width: 1
  save_checkpoints_steps: 5000
  save_summary_steps: 30
  train_steps: 80000
  maximum_features_length: 50
  maximum_labels_length: 50
  sample_buffer_size: 1000000  # Consider setting this to the training dataset size.
  keep_checkpoint_max: 10
  clip_gradients: 5.0

eval:
  eval_delay: 7200  # Every 5 hours.
  batch_size: 30
  save_eval_predictions: true
  external_evaluators: BLEU

infer:
  batch_size: 30
